{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\n",
    "2. https://pywavelets.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import scipy.io as sio\n",
    "import cv2\n",
    "import pywt\n",
    "import pywt.data\n",
    "import matplotlib.image as image\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from os.path import join, isdir, abspath, dirname\n",
    "import matplotlib.pyplot as plt\n",
    "# Customized import.\n",
    "from networks import HED\n",
    "from datasets import BsdsDataset\n",
    "from utils import Logger, AverageMeter, \\\n",
    "    load_checkpoint, save_checkpoint, load_vgg16_caffe, load_pretrained_caffe\n",
    "\n",
    "\n",
    "# # Parse arguments.\n",
    "# parser = argparse.ArgumentParser(description='HED training.')\n",
    "# # 1. Actions.\n",
    "# parser.add_argument('--test',             default=False,             help='Only test the model.', action='store_true')\n",
    "# # 2. Counts.\n",
    "# parser.add_argument('--train_batch_size', default=1,    type=int,   metavar='N', help='Training batch size.')\n",
    "# parser.add_argument('--test_batch_size',  default=1,    type=int,   metavar='N', help='Test batch size.')\n",
    "# parser.add_argument('--train_iter_size',  default=10,   type=int,   metavar='N', help='Training iteration size.')\n",
    "# parser.add_argument('--max_epoch',        default=40,   type=int,   metavar='N', help='Total epochs.')\n",
    "# parser.add_argument('--print_freq',       default=500,  type=int,   metavar='N', help='Print frequency.')\n",
    "# # 3. Optimizer settings.\n",
    "# parser.add_argument('--lr',               default=1e-6, type=float, metavar='F', help='Initial learning rate.')\n",
    "# parser.add_argument('--lr_stepsize',      default=1e4,  type=int,   metavar='N', help='Learning rate step size.')\n",
    "# # Note: Step size is based on number of iterations, not number of batches.\n",
    "# #   https://github.com/s9xie/hed/blob/94fb22f10cbfec8d84fbc0642b224022014b6bd6/src/caffe/solver.cpp#L498\n",
    "# parser.add_argument('--lr_gamma',         default=0.1,  type=float, metavar='F', help='Learning rate decay (gamma).')\n",
    "# parser.add_argument('--momentum',         default=0.9,  type=float, metavar='F', help='Momentum.')\n",
    "# parser.add_argument('--weight_decay',     default=2e-4, type=float, metavar='F', help='Weight decay.')\n",
    "# # 4. Files and folders.\n",
    "# parser.add_argument('--vgg16_caffe',      default='',                help='Resume VGG-16 Caffe parameters.')\n",
    "# parser.add_argument('--checkpoint',       default='',                help='Resume the checkpoint.')\n",
    "# parser.add_argument('--caffe_model',      default='',                help='Resume HED Caffe model.')\n",
    "# parser.add_argument('--output',           default='./output',        help='Output folder.')\n",
    "# parser.add_argument('--dataset',          default='./data/HED-BSDS', help='HED-BSDS dataset folder.')\n",
    "# # 5. Others.\n",
    "# parser.add_argument('--cpu',              default=False,             help='Enable CPU mode.', action='store_true')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# Set device.\n",
    "device = torch.device('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_trans_grayscale(images):\n",
    "    images = torchvision.utils.make_grid(images)\n",
    "    npimg = images.numpy()\n",
    "    img_np_right_dim = np.transpose(npimg, (1, 2, 0))\n",
    "    img_np_right_dim = cv2.cvtColor(img_np_right_dim, cv2.COLOR_BGR2GRAY)\n",
    "    coeffs2 = pywt.dwt2(img_np_right_dim, 'haar')\n",
    "    LL, (LH, HL, HH) = coeffs2\n",
    "    return([LL, LH, HL, HH])\n",
    "\n",
    "def WT_grayscale_path_input(path):  \n",
    "    \"\"\"Takes as input path string\"\"\"\n",
    "    orig_image = plt.imread(path)\n",
    "    if(len(orig_image.shape)==3):\n",
    "        gray_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = orig_image\n",
    "    coeffs2 = pywt.dwt2(gray_image, 'haar')\n",
    "    LL, (LH, HL, HH) = coeffs2\n",
    "    return [LL, LH, HL, HH]\n",
    "\n",
    "def im_save_path_modifier(input_path, mode):\n",
    "    \"\"\"\n",
    "    parameter: input_path is the path of the original image\n",
    "    parameter: mode is one of LL, LH, HL, HH\n",
    "    \"\"\"\n",
    "    dir_path = input_path[:16] + f'{mode}/' + input_path[16:input_path.rfind('/')]\n",
    "    image_path = input_path[:16] + f'{mode}/' + input_path[16:]\n",
    "    return(dir_path, image_path)\n",
    "\n",
    "def imshow(img):\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# II. Datasets.\n",
    "################################################\n",
    "# Datasets and dataloaders.\n",
    "\n",
    "train_dataset_LL = BsdsDataset(dataset_dir='./data/HED-BSDS/LL', split='train')\n",
    "\n",
    "train_dataset_LH = BsdsDataset(dataset_dir='./data/HED-BSDS/LH', split='train')\n",
    "\n",
    "train_dataset_HL = BsdsDataset(dataset_dir='./data/HED-BSDS/HL', split='train')\n",
    "\n",
    "train_dataset_HH = BsdsDataset(dataset_dir='./data/HED-BSDS/HH', split='train')\n",
    "\n",
    "train_loader_LL  = DataLoader(train_dataset_LL, batch_size=1,\n",
    "                           num_workers=4, drop_last=True, shuffle=False)\n",
    "\n",
    "train_loader_LH  = DataLoader(train_dataset_LH, batch_size=1,\n",
    "                           num_workers=4, drop_last=True, shuffle=False)\n",
    "\n",
    "train_loader_HL  = DataLoader(train_dataset_HL, batch_size=1,\n",
    "                           num_workers=4, drop_last=True, shuffle=False)\n",
    "\n",
    "train_loader_HH  = DataLoader(train_dataset_HH, batch_size=1,\n",
    "                           num_workers=4, drop_last=True, shuffle=False)\n",
    "\n",
    "\n",
    "# test_dataset  = BsdsDataset(dataset_dir='./data/HED-BSDS', split='test')\n",
    "\n",
    "# test_loader   = DataLoader(test_dataset,  batch_size=1,\n",
    "#                            num_workers=4, drop_last=False, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter_LL = iter(train_loader_LL)\n",
    "dataiter_LH = iter(train_loader_LH)\n",
    "dataiter_HL = iter(train_loader_HL)\n",
    "dataiter_HH = iter(train_loader_HH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: import 4 images from 4 dataloaders.\n",
    "\n",
    "images_LL, labels1 = dataiter_LL.next()\n",
    "images_LH, labels2 = dataiter_LH.next()\n",
    "images_HL, labels3 = dataiter_HL.next()\n",
    "images_HH, labels4 = dataiter_HH.next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that the images are the same\n",
    "torch.equal(labels3, labels4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.conv1_1.weight      lr:    1 decay:1\n",
      "module.conv1_1.bias        lr:    2 decay:0\n",
      "module.conv1_2.weight      lr:    1 decay:1\n",
      "module.conv1_2.bias        lr:    2 decay:0\n",
      "module.conv2_1.weight      lr:    1 decay:1\n",
      "module.conv2_1.bias        lr:    2 decay:0\n",
      "module.conv2_2.weight      lr:    1 decay:1\n",
      "module.conv2_2.bias        lr:    2 decay:0\n",
      "module.conv3_1.weight      lr:    1 decay:1\n",
      "module.conv3_1.bias        lr:    2 decay:0\n",
      "module.conv3_2.weight      lr:    1 decay:1\n",
      "module.conv3_2.bias        lr:    2 decay:0\n",
      "module.conv3_3.weight      lr:    1 decay:1\n",
      "module.conv3_3.bias        lr:    2 decay:0\n",
      "module.conv4_1.weight      lr:    1 decay:1\n",
      "module.conv4_1.bias        lr:    2 decay:0\n",
      "module.conv4_2.weight      lr:    1 decay:1\n",
      "module.conv4_2.bias        lr:    2 decay:0\n",
      "module.conv4_3.weight      lr:    1 decay:1\n",
      "module.conv4_3.bias        lr:    2 decay:0\n",
      "module.conv5_1.weight      lr:  100 decay:1\n",
      "module.conv5_1.bias        lr:  200 decay:0\n",
      "module.conv5_2.weight      lr:  100 decay:1\n",
      "module.conv5_2.bias        lr:  200 decay:0\n",
      "module.conv5_3.weight      lr:  100 decay:1\n",
      "module.conv5_3.bias        lr:  200 decay:0\n",
      "module.score_dsn1.weight   lr: 0.01 decay:1\n",
      "module.score_dsn1.bias     lr: 0.02 decay:0\n",
      "module.score_dsn2.weight   lr: 0.01 decay:1\n",
      "module.score_dsn2.bias     lr: 0.02 decay:0\n",
      "module.score_dsn3.weight   lr: 0.01 decay:1\n",
      "module.score_dsn3.bias     lr: 0.02 decay:0\n",
      "module.score_dsn4.weight   lr: 0.01 decay:1\n",
      "module.score_dsn4.bias     lr: 0.02 decay:0\n",
      "module.score_dsn5.weight   lr: 0.01 decay:1\n",
      "module.score_dsn5.bias     lr: 0.02 decay:0\n",
      "module.score_final.weight  lr:0.001 decay:1\n",
      "module.score_final.bias    lr:0.002 decay:0\n",
      "=> Start loading parameters...\n",
      "=> Loaded module.conv1_1.weight.\n",
      "=> Loaded module.conv1_1.bias.\n",
      "=> Loaded module.conv1_2.weight.\n",
      "=> Loaded module.conv1_2.bias.\n",
      "=> Loaded module.conv2_1.weight.\n",
      "=> Loaded module.conv2_1.bias.\n",
      "=> Loaded module.conv2_2.weight.\n",
      "=> Loaded module.conv2_2.bias.\n",
      "=> Loaded module.conv3_1.weight.\n",
      "=> Loaded module.conv3_1.bias.\n",
      "=> Loaded module.conv3_2.weight.\n",
      "=> Loaded module.conv3_2.bias.\n",
      "=> Loaded module.conv3_3.weight.\n",
      "=> Loaded module.conv3_3.bias.\n",
      "=> Loaded module.conv4_1.weight.\n",
      "=> Loaded module.conv4_1.bias.\n",
      "=> Loaded module.conv4_2.weight.\n",
      "=> Loaded module.conv4_2.bias.\n",
      "=> Loaded module.conv4_3.weight.\n",
      "=> Loaded module.conv4_3.bias.\n",
      "=> Loaded module.conv5_1.weight.\n",
      "=> Loaded module.conv5_1.bias.\n",
      "=> Loaded module.conv5_2.weight.\n",
      "=> Loaded module.conv5_2.bias.\n",
      "=> Loaded module.conv5_3.weight.\n",
      "=> Loaded module.conv5_3.bias.\n",
      "=> Finish loading parameters.\n"
     ]
    }
   ],
   "source": [
    "net = nn.DataParallel(HED(device))\n",
    "net.to(device);\n",
    "\n",
    "# Initialize the weights for HED model.\n",
    "def weights_init(m):\n",
    "    \"\"\" Weight initialization function. \"\"\"\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        # Initialize: m.weight.\n",
    "        if m.weight.data.shape == torch.Size([1, 5, 1, 1]):\n",
    "            # Constant initialization for fusion layer in HED network.\n",
    "            torch.nn.init.constant_(m.weight, 0.2)\n",
    "        else:\n",
    "            # Zero initialization following official repository.\n",
    "            # Reference: hed/docs/tutorial/layers.md\n",
    "            m.weight.data.zero_()\n",
    "        # Initialize: m.bias.\n",
    "        if m.bias is not None:\n",
    "            # Zero initialization.\n",
    "            m.bias.data.zero_()\n",
    "            \n",
    "\n",
    "net.apply(weights_init);\n",
    "net_parameters_id = defaultdict(list)\n",
    "# Optimizer settings.\n",
    "net_parameters_id = defaultdict(list)\n",
    "for name, param in net.named_parameters():\n",
    "    if name in ['module.conv1_1.weight', 'module.conv1_2.weight',\n",
    "                'module.conv2_1.weight', 'module.conv2_2.weight',\n",
    "                'module.conv3_1.weight', 'module.conv3_2.weight', 'module.conv3_3.weight',\n",
    "                'module.conv4_1.weight', 'module.conv4_2.weight', 'module.conv4_3.weight']:\n",
    "        print('{:26} lr:    1 decay:1'.format(name)); net_parameters_id['conv1-4.weight'].append(param)\n",
    "    elif name in ['module.conv1_1.bias', 'module.conv1_2.bias',\n",
    "                  'module.conv2_1.bias', 'module.conv2_2.bias',\n",
    "                  'module.conv3_1.bias', 'module.conv3_2.bias', 'module.conv3_3.bias',\n",
    "                  'module.conv4_1.bias', 'module.conv4_2.bias', 'module.conv4_3.bias']:\n",
    "        print('{:26} lr:    2 decay:0'.format(name)); net_parameters_id['conv1-4.bias'].append(param)\n",
    "    elif name in ['module.conv5_1.weight', 'module.conv5_2.weight', 'module.conv5_3.weight']:\n",
    "        print('{:26} lr:  100 decay:1'.format(name)); net_parameters_id['conv5.weight'].append(param)\n",
    "    elif name in ['module.conv5_1.bias', 'module.conv5_2.bias', 'module.conv5_3.bias'] :\n",
    "        print('{:26} lr:  200 decay:0'.format(name)); net_parameters_id['conv5.bias'].append(param)\n",
    "    elif name in ['module.score_dsn1.weight', 'module.score_dsn2.weight',\n",
    "                  'module.score_dsn3.weight', 'module.score_dsn4.weight', 'module.score_dsn5.weight']:\n",
    "        print('{:26} lr: 0.01 decay:1'.format(name)); net_parameters_id['score_dsn_1-5.weight'].append(param)\n",
    "    elif name in ['module.score_dsn1.bias', 'module.score_dsn2.bias',\n",
    "                  'module.score_dsn3.bias', 'module.score_dsn4.bias', 'module.score_dsn5.bias']:\n",
    "        print('{:26} lr: 0.02 decay:0'.format(name)); net_parameters_id['score_dsn_1-5.bias'].append(param)\n",
    "    elif name in ['module.score_final.weight']:\n",
    "        print('{:26} lr:0.001 decay:1'.format(name)); net_parameters_id['score_final.weight'].append(param)\n",
    "    elif name in ['module.score_final.bias']:\n",
    "        print('{:26} lr:0.002 decay:0'.format(name)); net_parameters_id['score_final.bias'].append(param)\n",
    "\n",
    "lr = 1e-6\n",
    "momentum = 0.9\n",
    "weight_decay = 2e-4\n",
    "lr_stepsize = 1e4\n",
    "lr_gamma = 0.1\n",
    "\n",
    "\n",
    "# Create optimizer.\n",
    "opt = torch.optim.SGD([\n",
    "    {'params': net_parameters_id['conv1-4.weight']      , 'lr': lr*1    , 'weight_decay': weight_decay},\n",
    "    {'params': net_parameters_id['conv1-4.bias']        , 'lr': lr*2    , 'weight_decay': 0.},\n",
    "    {'params': net_parameters_id['conv5.weight']        , 'lr': lr*100  , 'weight_decay': weight_decay},\n",
    "    {'params': net_parameters_id['conv5.bias']          , 'lr': lr*200  , 'weight_decay': 0.},\n",
    "    {'params': net_parameters_id['score_dsn_1-5.weight'], 'lr': lr*0.01 , 'weight_decay': weight_decay},\n",
    "    {'params': net_parameters_id['score_dsn_1-5.bias']  , 'lr': lr*0.02 , 'weight_decay': 0.},\n",
    "    {'params': net_parameters_id['score_final.weight']  , 'lr': lr*0.001, 'weight_decay': weight_decay},\n",
    "    {'params': net_parameters_id['score_final.bias']    , 'lr': lr*0.002, 'weight_decay': 0.},\n",
    "], lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "# Note: In train_val.prototxt and deploy.prototxt, the learning rates of score_final.weight/bias are different.\n",
    "\n",
    "# Learning rate scheduler.\n",
    "lr_schd = lr_scheduler.StepLR(opt, step_size=lr_stepsize, gamma=lr_gamma)\n",
    "\n",
    "\n",
    "################################################\n",
    "# IV. Pre-trained parameters.\n",
    "################################################\n",
    "# Load parameters from pre-trained VGG-16 Caffe model.\n",
    "load_vgg16_caffe(net, './data/5stage-vgg.py36pickle');\n",
    "\n",
    "# # Resume the checkpoint.\n",
    "# if args.checkpoint:\n",
    "#     load_checkpoint(net, opt, args.checkpoint)  # Omit the returned values.\n",
    "\n",
    "# # Resume the HED Caffe model.\n",
    "# if args.caffe_model:\n",
    "#     load_pretrained_caffe(net, args.caffe_model)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): HED(\n",
       "    (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(35, 35))\n",
       "    (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu): ReLU()\n",
       "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (score_dsn1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (score_dsn2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (score_dsn3): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (score_dsn4): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (score_dsn5): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (score_final): Conv2d(5, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net0 = net\n",
    "net1 = net\n",
    "net2 = net\n",
    "net3 = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt0 = opt\n",
    "opt1 = opt\n",
    "opt2 = opt\n",
    "opt3 = opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/home/24/524/cpatil/.local/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "opt.zero_grad()\n",
    "batch_loss_meter = AverageMeter()\n",
    "lr_schd.step()\n",
    "images_LL, labels1 = images_LL.to(device), labels1.to(device)\n",
    "images_LH, labels2 = images_LH.to(device), labels2.to(device)\n",
    "images_HL, labels3 = images_HL.to(device), labels3.to(device)\n",
    "images_HH, labels4 = images_HH.to(device), labels4.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list_LL = net(images_LL)\n",
    "preds_list_LH = net(images_LH)\n",
    "preds_list_HL = net(images_HL)\n",
    "preds_list_HH = net(images_HH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 192, 272])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_list_HH[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list = []\n",
    "for i in range(6):\n",
    "    LL = preds_list_LL[i].cpu().detach().numpy().reshape(192, 272)\n",
    "    LH = preds_list_LH[i].cpu().detach().numpy().reshape(192, 272)\n",
    "    HL = preds_list_HL[i].cpu().detach().numpy().reshape(192, 272)\n",
    "    HH = preds_list_HH[i].cpu().detach().numpy().reshape(192, 272)\n",
    "    full_image = pywt.idwt2((LL, (LH, HL, HH)), 'haar')\n",
    "    full_image = torch.reshape(torch.from_numpy(full_image), (1,1,384, 544))\n",
    "    full_image = full_image.to(device)\n",
    "    preds_list.append(full_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 384, 544])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss\n",
    "\n",
    "def weighted_cross_entropy_loss(preds, edges):\n",
    "    \"\"\" Calculate sum of weighted cross entropy loss. \"\"\"\n",
    "    # Reference:\n",
    "    #   hed/src/caffe/layers/sigmoid_cross_entropy_loss_layer.cpp\n",
    "    #   https://github.com/s9xie/hed/issues/7\n",
    "    mask = (edges > 0.5).float()\n",
    "    b, c, h, w = mask.shape\n",
    "    num_pos = torch.sum(mask, dim=[1, 2, 3]).float()  # Shape: [b,].\n",
    "    num_neg = c * h * w - num_pos                     # Shape: [b,].\n",
    "    weight = torch.zeros_like(mask)\n",
    "    weight[edges > 0.5]  = num_neg / (num_pos + num_neg)\n",
    "    weight[edges <= 0.5] = num_pos / (num_pos + num_neg)\n",
    "    # Calculate loss.\n",
    "    losses = torch.nn.functional.binary_cross_entropy(preds.float(), edges.float(), weight=weight, reduction='none')\n",
    "    loss   = torch.sum(losses) / b\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss = sum([weighted_cross_entropy_loss(preds, labels1) for preds in preds_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 544)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full_image.shape # assume this is the correct shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader_LL, train_loader_LH, train_loader_HL, train_loader_HH, net_LL, net_LH, net_HL, net_HH, opt_LL, opt_LH, opt_HL, opt_HH, lr_schd, epoch, save_dir_LL, save_dir_LH, save_dir_HL, save_dir_HH):\n",
    "    \"\"\" Training procedure. \"\"\"\n",
    "\n",
    "    dataiter_LL = iter(train_loader_LL)\n",
    "    dataiter_LH = iter(train_loader_LH)\n",
    "    dataiter_HL = iter(train_loader_HL)\n",
    "    dataiter_HH = iter(train_loader_HH)\n",
    "    \n",
    "    # Create the directory.\n",
    "    if not isdir(save_dir_LL):\n",
    "        os.makedirs(save_dir_LL)\n",
    "    # Create the directory.\n",
    "    if not isdir(save_dir_LH):\n",
    "        os.makedirs(save_dir_LH)\n",
    "    # Create the directory.\n",
    "    if not isdir(save_dir_HL):\n",
    "        os.makedirs(save_dir_HL)\n",
    "    # Create the directory.\n",
    "    if not isdir(save_dir_HH):\n",
    "        os.makedirs(save_dir_HH)\n",
    "        \n",
    "        \n",
    "    # Switch to train mode and clear the gradient.\n",
    "    net_LL.train()\n",
    "    opt_LL.zero_grad()\n",
    "    \n",
    "    net_LH.train()\n",
    "    opt_LH.zero_grad()\n",
    "    \n",
    "    net_HL.train()\n",
    "    opt_HL.zero_grad()\n",
    "    \n",
    "    net_HH.train()\n",
    "    opt_HH.zero_grad()\n",
    "    \n",
    "    \n",
    "    # Initialize meter and list.\n",
    "    batch_loss_meter = AverageMeter()\n",
    "    # Note: The counter is used here to record number of batches in current training iteration has been processed.\n",
    "    #       It aims to have large training iteration number even if GPU memory is not enough. However, such trick\n",
    "    #       can be used because batch normalization is not used in the network architecture.\n",
    "    \n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(tqdm(train_dataset.__len__())):\n",
    "        batch_index, (images_LL, edges) = dataiter_LL.next()\n",
    "        batch_index, (images_LH, edges) = dataiter_LH.next()\n",
    "        batch_index, (images_HL, edges) = dataiter_HL.next()\n",
    "        batch_index, (images_HH, edges) = dataiter_HH.next()\n",
    "        # Adjust learning rate and modify counter following Caffe's way.\n",
    "        if counter == 0:\n",
    "            lr_schd.step()  # Step at the beginning of the iteration.\n",
    "        counter += 1\n",
    "        # Get images and edges from current batch.\n",
    "        \n",
    "        images_LL, edges = images_LL.to(device), edges.to(device)\n",
    "        images_LH, edges = images_LH.to(device), edges.to(device)\n",
    "        images_HL, edges = images_HL.to(device), edges.to(device)\n",
    "        images_HH, edges = images_HH.to(device), edges.to(device)\n",
    "\n",
    "\n",
    "        # Generate predictions.\n",
    "        preds_list_LL = net_LL(images_LL)\n",
    "        preds_list_LH = net_LH(images_LH)\n",
    "        preds_list_HL = net_HL(images_HL)\n",
    "        preds_list_HH = net_LL(images_HH)\n",
    "        # Calculate the loss of current batch (sum of all scales and fused).\n",
    "        # Note: Here we mimic the \"iteration\" in official repository: iter_size batches will be considered together\n",
    "        #       to perform one gradient update. To achieve the goal, we calculate the equivalent iteration loss\n",
    "        #       eqv_iter_loss of current batch and generate the gradient. Then, instead of updating the weights,\n",
    "        #       we continue to calculate eqv_iter_loss and add the newly generated gradient to current gradient.\n",
    "        #       After iter_size batches, we will update the weights using the accumulated gradients and then zero\n",
    "        #       the gradients.\n",
    "        # Reference:\n",
    "        #   https://github.com/s9xie/hed/blob/94fb22f10cbfec8d84fbc0642b224022014b6bd6/src/caffe/solver.cpp#L230\n",
    "        #   https://www.zhihu.com/question/37270367\n",
    "        \n",
    "        # major change here\n",
    "        # use IDWT to generate image\n",
    "        # call that preds and pass it to loss function\n",
    "        \n",
    "        preds_list = []\n",
    "        for i in range(6):\n",
    "            LL = preds_list_LL[i].cpu().detach().numpy().reshape(192, 272)\n",
    "            LH = preds_list_LH[i].cpu().detach().numpy().reshape(192, 272)\n",
    "            HL = preds_list_HL[i].cpu().detach().numpy().reshape(192, 272)\n",
    "            HH = preds_list_HH[i].cpu().detach().numpy().reshape(192, 272)\n",
    "            full_image = pywt.idwt2((LL, (LH, HL, HH)), 'haar')\n",
    "            full_image = torch.reshape(torch.from_numpy(full_image), (1,1,384, 544))\n",
    "            full_image = full_image.to(device)\n",
    "            preds_list.append(full_image)\n",
    "        \n",
    "        batch_loss = sum([weighted_cross_entropy_loss(preds, edges) for preds in preds_list])\n",
    "        eqv_iter_loss = batch_loss / args.train_iter_size\n",
    "\n",
    "                \n",
    "        \n",
    "        \n",
    "        # Generate the gradient and accumulate (using equivalent average loss).\n",
    "        eqv_iter_loss.backward()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if counter == args.train_iter_size:\n",
    "            opt_LL.step()\n",
    "            opt_LH.step()\n",
    "            opt_HL.step()\n",
    "            opt_HH.step()\n",
    "            opt_LL.zero_grad()\n",
    "            opt_LH.zero_grad()\n",
    "            opt_HL.zero_grad()\n",
    "            opt_HH.zero_grad()\n",
    "            counter = 0  # Reset the counter.\n",
    "            \n",
    "        # Record loss.\n",
    "        batch_loss_meter.update(batch_loss.item())\n",
    "        \n",
    "        \n",
    "        # Log and save intermediate images.\n",
    "        if batch_index % args.print_freq == args.print_freq - 1:\n",
    "            # Log.\n",
    "            print(('Training epoch:{}/{}, batch:{}/{} current iteration:{}, ' +\n",
    "                   'current batch batch_loss:{}, epoch average batch_loss:{}, learning rate list:{}.').format(\n",
    "                   epoch, args.max_epoch, batch_index, len(train_loader_LL), lr_schd.last_epoch,\n",
    "                   batch_loss_meter.val, batch_loss_meter.avg, lr_schd.get_lr()))\n",
    "            # Generate intermediate images.\n",
    "            preds_list_and_edges = preds_list + [edges]\n",
    "            _, _, h, w = preds_list_and_edges[0].shape\n",
    "            interm_images = torch.zeros((len(preds_list_and_edges), 1, h, w))\n",
    "            for i in range(len(preds_list_and_edges)):\n",
    "                # Only fetch the first image in the batch.\n",
    "                interm_images[i, 0, :, :] = preds_list_and_edges[i][0, 0, :, :]\n",
    "            # Save the images.\n",
    "            torchvision.utils.save_image(interm_images, join(save_dir, 'batch-{}-1st-image.png'.format(batch_index)))\n",
    "    # Return the epoch average batch_loss.\n",
    "    return batch_loss_meter.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader_LL, test_loader_LH, test_loader_HL, test_loader_HH,  net_LL, net_LH, net_HL, net_HH, save_dir_LL, save_dir_LH, save_dir_HL, save_dir_HH):\n",
    "    \"\"\" Test procedure. \"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "    dataiter_test_LL = iter(test_loader_LL)\n",
    "    dataiter_test_LH = iter(test_loader_LH)\n",
    "    dataiter_test_HL = iter(test_loader_HL)\n",
    "    dataiter_test_HH = iter(test_loader_HH)\n",
    "\n",
    "\n",
    "    # Create the directories.\n",
    "    if not isdir(save_dir_LL):\n",
    "        os.makedirs(save_dir_LL)\n",
    "    if not isdir(save_dir_LH):\n",
    "        os.makedirs(save_dir_LH)\n",
    "    if not isdir(save_dir_HL):\n",
    "        os.makedirs(save_dir_HL)\n",
    "    if not isdir(save_dir_HH):\n",
    "        os.makedirs(save_dir_HH)\n",
    "        \n",
    "    save_png_dir_LL = join(save_dir_LL, 'png')\n",
    "    save_png_dir_LH = join(save_dir_LH, 'png')\n",
    "    save_png_dir_HL = join(save_dir_HL, 'png')\n",
    "    save_png_dir_HH = join(save_dir_HH, 'png')\n",
    "\n",
    "    \n",
    "    \n",
    "    if not isdir(save_png_dir_LL):\n",
    "        os.makedirs(save_png_dir_LL)\n",
    "    if not isdir(save_png_dir_LH):\n",
    "        os.makedirs(save_png_dir_LH)\n",
    "    if not isdir(save_png_dir_HL):\n",
    "        os.makedirs(save_png_dir_HL)\n",
    "    if not isdir(save_png_dir_HH):\n",
    "        os.makedirs(save_png_dir_HH)\n",
    "        \n",
    "    \n",
    "    save_mat_dir_LL = join(save_dir_LL, 'mat')\n",
    "    save_mat_dir_LH = join(save_dir_LH, 'mat')\n",
    "    save_mat_dir_HL = join(save_dir_HL, 'mat')\n",
    "    save_mat_dir_HH = join(save_dir_HH, 'mat')\n",
    "\n",
    "    \n",
    "    \n",
    "    if not isdir(save_mat_dir_LL):\n",
    "        os.makedirs(save_mat_dir_LL)\n",
    "    if not isdir(save_mat_dir_LH):\n",
    "        os.makedirs(save_mat_dir_LH)\n",
    "    if not isdir(save_mat_dir_HL):\n",
    "        os.makedirs(save_mat_dir_HL)\n",
    "    if not isdir(save_mat_dir_HH):\n",
    "        os.makedirs(save_mat_dir_HH)\n",
    "        \n",
    "        \n",
    "    # Switch to evaluation mode.\n",
    "    net_LL.eval()\n",
    "    net_LH.eval()\n",
    "    net_HL.eval()\n",
    "    net_HH.eval()\n",
    "    # Generate predictions and save.\n",
    "    assert args.test_batch_size == 1  # Currently only support test batch size 1.\n",
    "          \n",
    "\n",
    "        \n",
    "    for i in range(200): # hard-coding temporarily\n",
    "        batch_index, images_LL = dataiter_test_LL.next()\n",
    "        batch_index, images_LH = dataiter_test_LH.next()\n",
    "        batch_index, images_HL = dataiter_test_HL.next()\n",
    "        batch_index, images_HH = dataiter_test_HH.next()\n",
    "        \n",
    "        images_LL = images_LL.cuda()\n",
    "        images_LH = images_LH.cuda()\n",
    "        images_HL = images_HL.cuda()\n",
    "        images_HH = images_HH.cuda()\n",
    "        \n",
    "        _, _, h, w = images_LL.shape\n",
    "        \n",
    "        preds_list_LL = net_LL(images_LL)\n",
    "        preds_list_LH = net_LH(images_LH)\n",
    "        preds_list_HL = net_HL(images_HL)\n",
    "        preds_list_HH = net_HH(images_HH)\n",
    "        \n",
    "        preds_list = []\n",
    "        for i in range(6):\n",
    "            LL = preds_list_LL[i].cpu().detach().numpy().reshape(192, 272)\n",
    "            LH = preds_list_LH[i].cpu().detach().numpy().reshape(192, 272)\n",
    "            HL = preds_list_HL[i].cpu().detach().numpy().reshape(192, 272)\n",
    "            HH = preds_list_HH[i].cpu().detach().numpy().reshape(192, 272)\n",
    "            full_image = pywt.idwt2((LL, (LH, HL, HH)), 'haar')\n",
    "            full_image = torch.reshape(torch.from_numpy(full_image), (1,1,384, 544))\n",
    "            full_image = full_image.to(device)\n",
    "            preds_list.append(full_image)\n",
    "        \n",
    "        # generate preds_list (IDWT)\n",
    "        \n",
    "        fuse       = preds_list[-1].detach().cpu().numpy()[0, 0]  # Shape: [h, w].\n",
    "        name       = test_loader_LL.dataset.images_name[batch_index]\n",
    "        \n",
    "        sio.savemat(join(save_mat_dir_LL, '{}.mat'.format(name)), {'result': fuse})\n",
    "        sio.savemat(join(save_mat_dir_LH, '{}.mat'.format(name)), {'result': fuse})\n",
    "        sio.savemat(join(save_mat_dir_HL, '{}.mat'.format(name)), {'result': fuse})\n",
    "        sio.savemat(join(save_mat_dir_HH, '{}.mat'.format(name)), {'result': fuse})\n",
    "\n",
    "        Image.fromarray((fuse * 255).astype(np.uint8)).save(join(save_png_dir_LL, '{}.png'.format(name)))\n",
    "        Image.fromarray((fuse * 255).astype(np.uint8)).save(join(save_png_dir_LH, '{}.png'.format(name)))\n",
    "        Image.fromarray((fuse * 255).astype(np.uint8)).save(join(save_png_dir_HL, '{}.png'.format(name)))\n",
    "        Image.fromarray((fuse * 255).astype(np.uint8)).save(join(save_png_dir_HH, '{}.png'.format(name)))\n",
    "\n",
    "        # print('Test batch {}/{}.'.format(batch_index + 1, len(test_loader)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 12, 12])\n",
      "torch.Size([10, 5, 3, 34, 34])\n",
      "torch.Size([10, 5, 3, 19, 19])\n",
      "torch.Size([10, 5, 3, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_wavelets import DWTForward, DWTInverse # (or import DWT, IDWT)\n",
    "xfm = DWTForward(J=3, mode='zero', wave='db3')  # Accepts all wave types available to PyWavelets\n",
    "ifm = DWTInverse(mode='zero', wave='db3')\n",
    "X = torch.randn(10,5,64,64)\n",
    "Yl, Yh = xfm(X)\n",
    "print(Yl.shape)\n",
    "print(Yh[0].shape)\n",
    "print(Yh[1].shape)\n",
    "print(Yh[2].shape)\n",
    "Y = ifm((Yl, Yh))\n",
    "import numpy as np\n",
    "np.testing.assert_array_almost_equal(Y.cpu().numpy(), X.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer images and labels to GPU\n",
    "\n",
    "# pass images_LL, etc through their respective nets or just net\n",
    "# perform IDWT using pytorch_wavelets \n",
    "\n",
    "\n",
    "net.train()\n",
    "opt.zero_grad()\n",
    "batch_loss_meter = AverageMeter()\n",
    "lr_schd.step()\n",
    "images_LL, labels1 = images_LL.to(device), labels1.to(device)\n",
    "images_LH, labels2 = images_LH.to(device), labels2.to(device)\n",
    "images_HL, labels3 = images_HL.to(device), labels3.to(device)\n",
    "images_HH, labels4 = images_HH.to(device), labels4.to(device)\n",
    "\n",
    "\n",
    "preds_list_LL = net(images_LL)\n",
    "preds_list_LH = net(images_LH)\n",
    "preds_list_HL = net(images_HL)\n",
    "preds_list_HH = net(images_HH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list_LL_0 = preds_list_LL[0]\n",
    "preds_list_LH_0 = preds_list_LH[0]\n",
    "preds_list_HL_0 = preds_list_HL[0]\n",
    "preds_list_HH_0 = preds_list_HH[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifm = DWTInverse(mode='zero', wave='haar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-84e2319efa90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mifm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_list_LL_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_list_LH_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_list_HL_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_list_HH_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pytorch_wavelets/dwt/transform2d.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, coeffs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzeros\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0man\u001b[0m \u001b[0mefficient\u001b[0m \u001b[0mway\u001b[0m \u001b[0mthough\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \"\"\"\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0myl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoeffs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlowlevel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode_to_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "Y = ifm((preds_list_LL_0, preds_list_LH_0, preds_list_HL_0, preds_list_HH_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helloooo\n"
     ]
    }
   ],
   "source": [
    "print('Helloooo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
