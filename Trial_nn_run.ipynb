{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\n",
    "2. https://pywavelets.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import scipy.io as sio\n",
    "import cv2\n",
    "import pywt\n",
    "import pywt.data\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from os.path import join, isdir, abspath, dirname\n",
    "import matplotlib.pyplot as plt\n",
    "# Customized import.\n",
    "from networks import HED\n",
    "from datasets import BsdsDataset\n",
    "from utils import Logger, AverageMeter, \\\n",
    "    load_checkpoint, save_checkpoint, load_vgg16_caffe, load_pretrained_caffe\n",
    "\n",
    "\n",
    "# # Parse arguments.\n",
    "# parser = argparse.ArgumentParser(description='HED training.')\n",
    "# # 1. Actions.\n",
    "# parser.add_argument('--test',             default=False,             help='Only test the model.', action='store_true')\n",
    "# # 2. Counts.\n",
    "# parser.add_argument('--train_batch_size', default=1,    type=int,   metavar='N', help='Training batch size.')\n",
    "# parser.add_argument('--test_batch_size',  default=1,    type=int,   metavar='N', help='Test batch size.')\n",
    "# parser.add_argument('--train_iter_size',  default=10,   type=int,   metavar='N', help='Training iteration size.')\n",
    "# parser.add_argument('--max_epoch',        default=40,   type=int,   metavar='N', help='Total epochs.')\n",
    "# parser.add_argument('--print_freq',       default=500,  type=int,   metavar='N', help='Print frequency.')\n",
    "# # 3. Optimizer settings.\n",
    "# parser.add_argument('--lr',               default=1e-6, type=float, metavar='F', help='Initial learning rate.')\n",
    "# parser.add_argument('--lr_stepsize',      default=1e4,  type=int,   metavar='N', help='Learning rate step size.')\n",
    "# # Note: Step size is based on number of iterations, not number of batches.\n",
    "# #   https://github.com/s9xie/hed/blob/94fb22f10cbfec8d84fbc0642b224022014b6bd6/src/caffe/solver.cpp#L498\n",
    "# parser.add_argument('--lr_gamma',         default=0.1,  type=float, metavar='F', help='Learning rate decay (gamma).')\n",
    "# parser.add_argument('--momentum',         default=0.9,  type=float, metavar='F', help='Momentum.')\n",
    "# parser.add_argument('--weight_decay',     default=2e-4, type=float, metavar='F', help='Weight decay.')\n",
    "# # 4. Files and folders.\n",
    "# parser.add_argument('--vgg16_caffe',      default='',                help='Resume VGG-16 Caffe parameters.')\n",
    "# parser.add_argument('--checkpoint',       default='',                help='Resume the checkpoint.')\n",
    "# parser.add_argument('--caffe_model',      default='',                help='Resume HED Caffe model.')\n",
    "# parser.add_argument('--output',           default='./output',        help='Output folder.')\n",
    "# parser.add_argument('--dataset',          default='./data/HED-BSDS', help='HED-BSDS dataset folder.')\n",
    "# # 5. Others.\n",
    "# parser.add_argument('--cpu',              default=False,             help='Enable CPU mode.', action='store_true')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# Set device.\n",
    "device = torch.device('cuda')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_trans_grayscale(images):\n",
    "    images = torchvision.utils.make_grid(images)\n",
    "    npimg = images.numpy()\n",
    "    img_np_right_dim = np.transpose(npimg, (1, 2, 0))\n",
    "    img_np_right_dim = cv2.cvtColor(img_np_right_dim, cv2.COLOR_BGR2GRAY)\n",
    "    coeffs2 = pywt.dwt2(img_np_right_dim, 'haar')\n",
    "    LL, (LH, HL, HH) = coeffs2\n",
    "    return([LL, LH, HL, HH])\n",
    "\n",
    "def im_save_path_modifier(input_path, mode):\n",
    "    \"\"\"\n",
    "    parameter: input_path is the path of the original image\n",
    "    parameter: mode is one of LL, LH, HL, HH\n",
    "    \"\"\"\n",
    "    dir_path = input_path[:16] + f'{mode}/' + input_path[16:input_path.rfind('/')]\n",
    "    image_path = input_path[:16] + f'{mode}/' + input_path[16:]\n",
    "    return(dir_path, image_path)\n",
    "\n",
    "def imshow(img):\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_dir = abspath(dirname(__file__))\n",
    "# output_dir = join(current_dir, args.output)\n",
    "# if not isdir(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# # Set logger.\n",
    "# now_str = datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "# log = Logger(join(output_dir, 'log-{}.txt'.format(now_str)))\n",
    "# sys.stdout = log  # Overwrite the standard output.\n",
    "\n",
    "################################################\n",
    "# II. Datasets.\n",
    "################################################\n",
    "# Datasets and dataloaders.\n",
    "train_dataset = BsdsDataset(dataset_dir='./data/HED-BSDS', split='train')\n",
    "test_dataset  = BsdsDataset(dataset_dir='./data/HED-BSDS', split='test')\n",
    "train_loader  = DataLoader(train_dataset, batch_size=1,\n",
    "                           num_workers=4, drop_last=True, shuffle=True)\n",
    "test_loader   = DataLoader(test_dataset,  batch_size=1,\n",
    "                           num_workers=4, drop_last=False, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28800"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "object too deep for desired array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d8b6a3746eed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mlabel_dir_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim_save_path_modifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_trans_imlist_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_dir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, **kwargs)\u001b[0m\n\u001b[1;32m   2064\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2066\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"png\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpil_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m                 \u001b[0m_png\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: object too deep for desired array"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels, path, label_path = dataiter.next()\n",
    "\n",
    "wav_trans_imlist = wav_trans_grayscale(images)\n",
    "wav_trans_imlist_names  = ['LL', 'LH', 'HL', 'HH']\n",
    "\n",
    "for i in range(len(wav_trans_imlist)):\n",
    "    \n",
    "    # saving image\n",
    "    \n",
    "    dir_path, image_path = im_save_path_modifier(path[0], str(wav_trans_imlist_names[i]))\n",
    "    os.makedirs(dir_path)\n",
    "    plt.imsave(image_path, wav_trans_imlist[i], cmap = 'gray')\n",
    "    \n",
    "    # saving label\n",
    "    \n",
    "    label_dir_path, label_path = im_save_path_modifier(label_path[0], str(wav_trans_imlist_names[i]))\n",
    "    os.makedirs(label_dir_path)\n",
    "    plt.imsave(label_path, labels, cmap = 'gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should convert to numpy before saving images and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-104.00699  ,    0.9930121,   12.993012 , ...,   46.99301  ,\n",
       "            35.99301  ,   23.993011 ],\n",
       "         [-104.00699  ,   -2.006988 ,   10.993012 , ...,   45.99301  ,\n",
       "            33.99301  ,   51.99301  ],\n",
       "         [-104.00699  ,   -7.006988 ,    8.993012 , ...,   45.99301  ,\n",
       "            25.993011 ,   52.99301  ],\n",
       "         ...,\n",
       "         [-104.00699  ,  -87.00699  ,  -90.00699  , ...,  -56.00699  ,\n",
       "           -73.00699  ,  -66.00699  ],\n",
       "         [-104.00699  ,  -87.00699  ,  -90.00699  , ...,  -72.00699  ,\n",
       "           -69.00699  ,  -59.00699  ],\n",
       "         [-104.00699  ,  -84.00699  ,  -87.00699  , ...,  -60.00699  ,\n",
       "           -70.00699  ,  -74.00699  ]],\n",
       "\n",
       "        [[-106.66877  ,   14.331232 ,   30.331232 , ...,   75.33123  ,\n",
       "            64.33123  ,   51.331234 ],\n",
       "         [-111.66877  ,   11.331232 ,   28.331232 , ...,   74.33123  ,\n",
       "            62.331234 ,   79.33123  ],\n",
       "         [-116.66877  ,    6.3312325,   26.331232 , ...,   74.33123  ,\n",
       "            54.331234 ,   80.33123  ],\n",
       "         ...,\n",
       "         [-115.66877  ,  -91.66877  ,  -94.66877  , ...,  -35.668766 ,\n",
       "           -53.668766 ,  -50.668766 ],\n",
       "         [-115.66877  ,  -91.66877  ,  -94.66877  , ...,  -51.668766 ,\n",
       "           -49.668766 ,  -45.668766 ],\n",
       "         [-114.66877  ,  -88.66877  ,  -91.66877  , ...,  -39.668766 ,\n",
       "           -50.668766 ,  -60.668766 ]],\n",
       "\n",
       "        [[ -96.67892  ,   24.321085 ,   41.321087 , ...,   94.32108  ,\n",
       "            83.32108  ,   73.32108  ],\n",
       "         [-101.67892  ,   21.321085 ,   39.321087 , ...,   93.32108  ,\n",
       "            81.32108  ,  101.32108  ],\n",
       "         [-107.67892  ,   16.321085 ,   37.321087 , ...,   93.32108  ,\n",
       "            73.32108  ,  102.32108  ],\n",
       "         ...,\n",
       "         [-121.67892  ,  -97.67892  , -100.67892  , ...,  -26.678915 ,\n",
       "           -46.678913 ,  -45.678913 ],\n",
       "         [-121.67892  ,  -97.67892  , -100.67892  , ...,  -42.678913 ,\n",
       "           -42.678913 ,  -39.678913 ],\n",
       "         [-120.67892  ,  -94.67892  ,  -97.67892  , ...,  -30.678915 ,\n",
       "           -43.678913 ,  -54.678913 ]]]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing:  \n",
    "Option 1:\n",
    "\n",
    "1. Take wavelet transform of every channel\n",
    "2. Stack up RGB channels after performing wavelet transform\n",
    "3. Feed this to network.\n",
    "\n",
    "Option 2:\n",
    "\n",
    "1. Take grayscale of the input image\n",
    "2. Pass this to network.  \n",
    "Note: For option 2, we will have to change the first layer of the network to suit our changed dimensions of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize = [10,10])\n",
    "# ax1 = fig.add_subplot(221)\n",
    "# ax1.imshow(LL, cmap = 'gray')\n",
    "# ax1.set_title('LL')\n",
    "# ax2 = fig.add_subplot(222)\n",
    "# ax2.imshow(LH, cmap = 'gray')\n",
    "# ax2.set_title('LH')\n",
    "# ax3 = fig.add_subplot(223)\n",
    "# ax3.imshow(HL, cmap = 'gray')\n",
    "# ax3.set_title('HL')\n",
    "# ax4 = fig.add_subplot(224)\n",
    "# ax4.imshow(HH, cmap = 'gray')\n",
    "# ax4.set_title('HH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a function that does this - Done!\n",
    "2. Create 4 datasets\n",
    "3. Train 4 networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2:\n",
    "\n",
    "We need to:\n",
    "\n",
    "1. Load image from path normal/p1  \n",
    "2. Preprocess image\n",
    "3. Save it in LL/p1, LH/p1, HL/p1, and HH/p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use os.mkdir for one image - Done!\n",
    "2. Do it for every mode. - Done!\n",
    "3. Do it for every image. - Pending!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks for 11/28\n",
    "1. solve np shape error\n",
    "2. do it for every image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
