{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\n",
    "2. https://pywavelets.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import scipy.io as sio\n",
    "import cv2\n",
    "import pywt\n",
    "import pywt.data\n",
    "import matplotlib.image as image\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from os.path import join, isdir, abspath, dirname\n",
    "import matplotlib.pyplot as plt\n",
    "# Customized import.\n",
    "from networks import HED\n",
    "from datasets import BsdsDataset\n",
    "from utils import Logger, AverageMeter, \\\n",
    "    load_checkpoint, save_checkpoint, load_vgg16_caffe, load_pretrained_caffe\n",
    "\n",
    "\n",
    "# # Parse arguments.\n",
    "# parser = argparse.ArgumentParser(description='HED training.')\n",
    "# # 1. Actions.\n",
    "# parser.add_argument('--test',             default=False,             help='Only test the model.', action='store_true')\n",
    "# # 2. Counts.\n",
    "# parser.add_argument('--train_batch_size', default=1,    type=int,   metavar='N', help='Training batch size.')\n",
    "# parser.add_argument('--test_batch_size',  default=1,    type=int,   metavar='N', help='Test batch size.')\n",
    "# parser.add_argument('--train_iter_size',  default=10,   type=int,   metavar='N', help='Training iteration size.')\n",
    "# parser.add_argument('--max_epoch',        default=40,   type=int,   metavar='N', help='Total epochs.')\n",
    "# parser.add_argument('--print_freq',       default=500,  type=int,   metavar='N', help='Print frequency.')\n",
    "# # 3. Optimizer settings.\n",
    "# parser.add_argument('--lr',               default=1e-6, type=float, metavar='F', help='Initial learning rate.')\n",
    "# parser.add_argument('--lr_stepsize',      default=1e4,  type=int,   metavar='N', help='Learning rate step size.')\n",
    "# # Note: Step size is based on number of iterations, not number of batches.\n",
    "# #   https://github.com/s9xie/hed/blob/94fb22f10cbfec8d84fbc0642b224022014b6bd6/src/caffe/solver.cpp#L498\n",
    "# parser.add_argument('--lr_gamma',         default=0.1,  type=float, metavar='F', help='Learning rate decay (gamma).')\n",
    "# parser.add_argument('--momentum',         default=0.9,  type=float, metavar='F', help='Momentum.')\n",
    "# parser.add_argument('--weight_decay',     default=2e-4, type=float, metavar='F', help='Weight decay.')\n",
    "# # 4. Files and folders.\n",
    "# parser.add_argument('--vgg16_caffe',      default='',                help='Resume VGG-16 Caffe parameters.')\n",
    "# parser.add_argument('--checkpoint',       default='',                help='Resume the checkpoint.')\n",
    "# parser.add_argument('--caffe_model',      default='',                help='Resume HED Caffe model.')\n",
    "# parser.add_argument('--output',           default='./output',        help='Output folder.')\n",
    "# parser.add_argument('--dataset',          default='./data/HED-BSDS', help='HED-BSDS dataset folder.')\n",
    "# # 5. Others.\n",
    "# parser.add_argument('--cpu',              default=False,             help='Enable CPU mode.', action='store_true')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# Set device.\n",
    "device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_trans_grayscale(images):\n",
    "    images = torchvision.utils.make_grid(images)\n",
    "    npimg = images.numpy()\n",
    "    img_np_right_dim = np.transpose(npimg, (1, 2, 0))\n",
    "    img_np_right_dim = cv2.cvtColor(img_np_right_dim, cv2.COLOR_BGR2GRAY)\n",
    "    coeffs2 = pywt.dwt2(img_np_right_dim, 'haar')\n",
    "    LL, (LH, HL, HH) = coeffs2\n",
    "    return([LL, LH, HL, HH])\n",
    "\n",
    "def WT_grayscale_path_input(path):  \n",
    "    \"\"\"Takes as input path string\"\"\"\n",
    "    orig_image = plt.imread(path)\n",
    "    if(len(orig_image.shape)==3):\n",
    "        gray_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = orig_image\n",
    "    coeffs2 = pywt.dwt2(gray_image, 'haar')\n",
    "    LL, (LH, HL, HH) = coeffs2\n",
    "    return [LL, LH, HL, HH]\n",
    "\n",
    "def im_save_path_modifier(input_path, mode):\n",
    "    \"\"\"\n",
    "    parameter: input_path is the path of the original image\n",
    "    parameter: mode is one of LL, LH, HL, HH\n",
    "    \"\"\"\n",
    "    dir_path = input_path[:16] + f'{mode}/' + input_path[16:input_path.rfind('/')]\n",
    "    image_path = input_path[:16] + f'{mode}/' + input_path[16:]\n",
    "    return(dir_path, image_path)\n",
    "\n",
    "def imshow(img):\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_dir = abspath(dirname(__file__))\n",
    "# output_dir = join(current_dir, args.output)\n",
    "# if not isdir(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# # Set logger.\n",
    "# now_str = datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "# log = Logger(join(output_dir, 'log-{}.txt'.format(now_str)))\n",
    "# sys.stdout = log  # Overwrite the standard output.\n",
    "\n",
    "################################################\n",
    "# II. Datasets.\n",
    "################################################\n",
    "# Datasets and dataloaders.\n",
    "train_dataset = BsdsDataset(dataset_dir='./data/HED-BSDS', split='train')\n",
    "test_dataset  = BsdsDataset(dataset_dir='./data/HED-BSDS', split='test')\n",
    "train_loader  = DataLoader(train_dataset, batch_size=1,\n",
    "                           num_workers=4, drop_last=True, shuffle=True)\n",
    "test_loader   = DataLoader(test_dataset,  batch_size=1,\n",
    "                           num_workers=4, drop_last=False, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28800"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-08d8b491895d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwav_trans_imlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWT_grayscale_path_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwav_trans_imlist_names\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'LL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LH'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HH'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    829\u001b[0m                 \u001b[0;31m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0;31m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "images, labels, path, label_path = dataiter.next()\n",
    "\n",
    "wav_trans_imlist = WT_grayscale_path_input(path[0])\n",
    "wav_trans_imlist_names  = ['LL', 'LH', 'HL', 'HH']\n",
    "\n",
    "\n",
    "for i in range(len(wav_trans_imlist)):\n",
    "    \n",
    "    # saving image\n",
    "    \n",
    "    dir_path, image_path = im_save_path_modifier(path[0], str(wav_trans_imlist_names[i]))\n",
    "    os.makedirs(dir_path)\n",
    "    plt.imsave(image_path, wav_trans_imlist[i], cmap = 'gray')\n",
    "    \n",
    "    # saving label\n",
    "    # retrieve label image\n",
    "    im_label = plt.imread(label_path[0])\n",
    "    label_dir_path_new, label_path_new = im_save_path_modifier(label_path[0], str(wav_trans_imlist_names[i]))\n",
    "    os.makedirs(label_dir_path_new)\n",
    "    plt.imsave(label_path_new, im_label, cmap = 'gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sad_images = []\n",
    "sad_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cd6b2a3f6169>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "for j, data in tqdm(enumerate(train_loader, 0)):\n",
    "    \n",
    "    images, path = data\n",
    "    \n",
    "    print(path[0])\n",
    "\n",
    "#     wav_trans_imlist = WT_grayscale_path_input(path[0])\n",
    "#     wav_trans_imlist_names  = ['LL', 'LH', 'HL', 'HH']\n",
    "\n",
    "\n",
    "#     for i in range(len(wav_trans_imlist)):\n",
    "\n",
    "#         # saving image\n",
    "\n",
    "#         dir_path, image_path = im_save_path_modifier(path[0], str(wav_trans_imlist_names[i]))\n",
    "#         os.makedirs(dir_path, exist_ok=True)\n",
    "#         plt.imsave(image_path, wav_trans_imlist[i], cmap = 'gray')\n",
    "\n",
    "#         # saving label\n",
    "#         # retrieve label image\n",
    "#         im_label = plt.imread(label_path[0])\n",
    "#         label_dir_path_new, label_path_new = im_save_path_modifier(label_path[0], str(wav_trans_imlist_names[i]))\n",
    "#         os.makedirs(label_dir_path_new, exist_ok=True)\n",
    "#         plt.imsave(label_path_new, im_label, cmap = 'gray')\n",
    "        \n",
    "#     if(j%1000==0):\n",
    "#         print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15684"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sad_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: Instead of taking image from the dataloader, we take the image path from it and get the image ourselves from the path. So we don't have to deal with torch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should convert to numpy before saving images and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing:  \n",
    "Option 1:\n",
    "\n",
    "1. Take wavelet transform of every channel\n",
    "2. Stack up RGB channels after performing wavelet transform\n",
    "3. Feed this to network.\n",
    "\n",
    "Option 2:\n",
    "\n",
    "1. Take grayscale of the input image\n",
    "2. Pass this to network.  \n",
    "Note: For option 2, we will have to change the first layer of the network to suit our changed dimensions of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3f2ce61540d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m221\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m222\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LL' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = [10,10])\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1.imshow(LL, cmap = 'gray')\n",
    "ax1.set_title('LL')\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax2.imshow(LH, cmap = 'gray')\n",
    "ax2.set_title('LH')\n",
    "ax3 = fig.add_subplot(223)\n",
    "ax3.imshow(HL, cmap = 'gray')\n",
    "ax3.set_title('HL')\n",
    "ax4 = fig.add_subplot(224)\n",
    "ax4.imshow(HH, cmap = 'gray')\n",
    "ax4.set_title('HH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a function that does this - Done!\n",
    "2. Create 4 datasets\n",
    "3. Train 4 networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2:\n",
    "\n",
    "We need to:\n",
    "\n",
    "1. Load image from path normal/p1  \n",
    "2. Preprocess image\n",
    "3. Save it in LL/p1, LH/p1, HL/p1, and HH/p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use os.mkdir for one image - Done!\n",
    "2. Do it for every mode. - Done!\n",
    "3. Do it for every image. - Pending!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks for 11/28\n",
    "1. solve np shape error\n",
    "2. do it for every image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided not to do the wavelet transform for labels, because what's the point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dec 7 Meeting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. calculate loss for 1 image  \n",
    "    a. outputs of all neural networks  \n",
    "    b. Perform IDWT and combine them into final image  \n",
    "    c. Use weighted_cross_entropy_loss to calculate loss.  \n",
    "\n",
    "2. Then worry about backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
